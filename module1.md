The big innovation that unlocked the power of Large Language Model was something called the "attention mechanism". 

**Attention as the word implies, allows a computer, or transformer in this case, to see exactly how one word relates to the others in a certain sequence**. It gives a score of how important each of the word is in a sequence to each other. To you and me this seems like an obvious concept, it's something that we developed early on in life but it's vitally important piece for natural language processing to unlock abilities that it wasn't able to achieve before.

# Attention is (not) all you need
Now while attention was a huge step forward in our abilities to master natural language processing, it was actually only one piece out of a few that were required to 

![image](https://github.com/vivekprm/LLM-FoundationModels/assets/2403660/767db0da-cbfe-4dbd-922f-2b93be976e4a)

build the tranformers and the models that we see these days.

# The Transformer Block
