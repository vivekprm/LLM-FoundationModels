{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-sandbox\n",
        "# MAGIC\n",
        "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
        "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab: Diving into the Transformer - Utilizing embeddings from transformers\n",
        "# MAGIC\n",
        "In this lab, we will delve into the workings of the Transformer's encoder. We will build the components needed to create our encoder-based model and investigate the embeddings it produces. You will then be asked questions related to these embeddings, including comparisons, distance measurements, and the utilization of masked language modeling (MLM).\n",
        "# MAGIC\n",
        "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
        "1. Develop an encoder from scratch\n",
        "2. Investigating word embeddings (from our untrained models, and BERT)\n",
        "4. Gain practice in Masked Language Modeling (MLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classroom Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MAGIC %run ../Includes/Classroom-Setup"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 1 - Building our own Encoder Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Feed Forward Network\n",
        "# MAGIC\n",
        "We begin by defining the FeedForward class, which represents a simple feed-forward neural network with two linear layers separated by a ReLU activation function, and a dropout layer for regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden_dim, dropout = 0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Transformer Encoder Block\n",
        "# MAGIC\n",
        "The TransformerEncoderBlock class represents a single block of the transformer encoder, which consists of a multi-head self-attention layer and a feed-forward neural network, with layer normalization and residual connections applied to the outputs of each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, conv_hidden_dim, dropout=0.1):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.feed_forward = FeedForward(d_model, conv_hidden_dim, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Multi-Head Attention\n",
        "        attn_output, _ = self.attention(x, x, x, attn_mask=mask)\n",
        "        x = x + self.dropout(attn_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Feed Forward Network\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Transformer Encoder\n",
        "# MAGIC\n",
        "The TransformerEncoder class represents the complete transformer encoder, which consists of a word embedding layer, a positional encoding layer, and a series of transformer encoder blocks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_heads, conv_hidden_dim, num_layers, dropout=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.word_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(1000, d_model)  # Assuming a maximum sequence length of 1000\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerEncoderBlock(d_model, num_heads, conv_hidden_dim, dropout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        seq_length = x.shape[1]\n",
        "        positions = torch.arange(0, seq_length).expand(x.shape[0], seq_length).to(x.device)\n",
        "        out = self.word_embedding(x) + self.position_embedding(positions)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, mask)\n",
        "\n",
        "        return out\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instantiate the Model and Perform a Forward Pass\n",
        "# MAGIC\n",
        "We now instantiate the model with a specific set of hyperparameters, generate some random input data, and perform a forward pass through the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Assume the following hyperparameters\n",
        "vocab_size = 5000  # size of the vocabulary\n",
        "d_model = 512  # dimension of the word embedding\n",
        "num_heads = 8  # number of attention heads\n",
        "conv_hidden_dim = 2048  # dimension of the hidden layer in the feed-forward network\n",
        "num_layers = 6  # number of Transformer Encoder blocks\n",
        "dropout = 0.1  # dropout rate\n",
        "\n",
        "# Instantiate the model\n",
        "model = TransformerEncoder(vocab_size, d_model, num_heads, conv_hidden_dim, num_layers, dropout)\n",
        "\n",
        "# Generate some example input\n",
        "input_tensor = torch.randint(0, vocab_size, (1, 20))  # batch size of 1 and sequence length of 20\n",
        "\n",
        "# Forward pass through the model\n",
        "output = model(input_tensor, mask=None)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Investigate Word Embeddings\n",
        "# MAGIC\n",
        "We now generate some random input data and perform a forward pass through the model to obtain the embeddings for each word in the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate some example input\n",
        "input_tensor = torch.randint(0, vocab_size, (1, 5))  # batch size of 1 and sequence length of 5\n",
        "\n",
        "# Forward pass through the model\n",
        "embeddings = model(input_tensor, mask=None)\n",
        "\n",
        "# The `embeddings` tensor now contains the contextualized embeddings for each word in the input sequence\n",
        "print(embeddings)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Vocabulary and Word-to-ID Mapping\n",
        "# MAGIC\n",
        "To interpret the input and output of our model, we define a vocabulary and a mapping from words to their corresponding IDs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Let's assume the following is our vocabulary\n",
        "vocabulary = [\n",
        "    \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \n",
        "    \"the\", \"a\", \"an\", \n",
        "    \"cat\", \"dog\", \"fish\", \"bird\", \"lion\", \"tiger\", \"elephant\", \"monkey\",\n",
        "    \"runs\", \"jumps\", \"sleeps\", \"eats\", \"drinks\",\n",
        "    \"fast\", \"slow\", \"big\", \"small\", \"red\", \"green\", \"blue\", \"yellow\",\n",
        "    \"is\", \"was\", \"will\", \"can\", \"has\", \"have\", \"had\", \"do\", \"does\",\n",
        "    \"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\",\n",
        "    \"my\", \"your\", \"his\", \"her\", \"its\", \"our\", \"their\"\n",
        "]\n",
        "\n",
        "# Create a word2id dictionary\n",
        "word2id = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "\n",
        "# Print the dictionary\n",
        "print(word2id)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Measure Sentence Similarity\n",
        "# MAGIC\n",
        "We define a function to measure the cosine similarity between two sentences, which is done by averaging the embeddings of the words in each sentence and then calculating the cosine similarity between these average embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define a simple function to calculate cosine similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    vec1 = vec1.squeeze()  # remove dimensions of size 1\n",
        "    vec2 = vec2.squeeze()  # remove dimensions of size 1\n",
        "    return torch.dot(vec1, vec2) / (torch.norm(vec1) * torch.norm(vec2))\n",
        "\n",
        "# Define a function to convert a sentence into embeddings\n",
        "def sentence_to_embeddings(sentence, model, word2id):\n",
        "    # Convert sentence to token ids\n",
        "    input_tensor = torch.tensor([word2id.get(word, word2id[\"[UNK]\"]) for word in sentence.split()], dtype=torch.long).unsqueeze(0)\n",
        "    embeddings = model(input_tensor, mask=None)\n",
        "    return embeddings\n",
        "\n",
        "# Define a function to compute the similarity between two sentences\n",
        "def sentence_similarity(sentence1, sentence2, model, word2id):\n",
        "    embeddings1 = sentence_to_embeddings(sentence1, model, word2id)\n",
        "    embeddings2 = sentence_to_embeddings(sentence2, model, word2id)\n",
        "\n",
        "    # Compute the average embeddings of each sentence\n",
        "    avg_embedding1 = torch.mean(embeddings1, dim=1)\n",
        "    avg_embedding2 = torch.mean(embeddings2, dim=1)\n",
        "\n",
        "    # Compute and return the cosine similarity\n",
        "    return cosine_similarity(avg_embedding1, avg_embedding2)\n",
        "\n",
        "# Now we can compute the similarity between two sentences\n",
        "sentence1 = \"the cat has a blue fish\"\n",
        "sentence2 = \"my sister's dog sleeps\"\n",
        "# Compute the similarity\n",
        "similarity = sentence_similarity(sentence1, sentence2, model, word2id)\n",
        "\n",
        "# Extract the value from the tensor and convert it to a Python float\n",
        "similarity_score = similarity.item()\n",
        "\n",
        "# Print the result with a descriptive sentence\n",
        "print(f\"The cosine similarity between the sentences '{sentence1}' and '{sentence2}' is {similarity_score:.2f}.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Word Embeddings\n",
        "# MAGIC\n",
        "We define a function to visualize the embeddings of a set of words in 2D space, which is done by using PCA to reduce the dimensionality of the embeddings to 2 and then plotting them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a list of words to plot\n",
        "words = [\n",
        "    # Animals\n",
        "    \"cat\", \"dog\", \"fish\", \"bird\", \"lion\", \"tiger\", \"elephant\", \"monkey\",\n",
        "    # Colors\n",
        "    \"red\", \"green\", \"blue\", \"yellow\", \"black\", \"white\", \"pink\", \"orange\",\n",
        "    # Verbs\n",
        "    \"run\", \"jump\", \"swim\", \"fly\", \"eat\", \"drink\", \"sleep\", \"play\"\n",
        "]\n",
        "# Create artificial embeddings for demonstration purposes\n",
        "embeddings = np.array([np.random.rand(512) for _ in words])  # 512 is the dimension of the embeddings\n",
        "\n",
        "# Use PCA to reduce the dimensionality of the embeddings to 2\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "# Plot the embeddings\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],s=500)\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
        "plt.show()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare with Pre-Trained BERT Embeddings\n",
        "# MAGIC\n",
        "We load a pre-trained BERT model, generate embeddings for a set of words, and visualize them in the same way as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import necessary libraries\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "model_bert = BertModel.from_pretrained(model_name, cache_dir=DA.paths.datasets+\"/models\")\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=DA.paths.datasets+\"/models\")\n",
        "\n",
        "# Define a list of words to plot\n",
        "words = [\n",
        "    # Animals\n",
        "    \"cat\", \"dog\", \"fish\", \"bird\", \"lion\", \"tiger\", \"elephant\", \"monkey\",\n",
        "    # Colors\n",
        "    \"red\", \"green\", \"blue\", \"yellow\", \"black\", \"white\", \"pink\", \"orange\",\n",
        "    # Verbs\n",
        "    \"run\", \"jump\", \"swim\", \"fly\", \"eat\", \"drink\", \"sleep\", \"play\"\n",
        "]\n",
        "# Get the embeddings of the words\n",
        "embeddings = []\n",
        "for word in words:\n",
        "    inputs = tokenizer(word, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model_bert(**inputs)\n",
        "    embeddings.append(outputs.last_hidden_state[0, 0, :].numpy())\n",
        "\n",
        "# Use PCA to reduce the dimensionality of the embeddings to 2\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "# Plot the embeddings\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],s=500)\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 1 - Questions\n",
        "# MAGIC\n",
        "Having built our transformer encoder and inspected the embeddings it produces, we can now explore some questions related to these embeddings.\n",
        "# MAGIC\n",
        "**Question 1:** \n",
        "How does changing different hyperparameters (e.g., the dimension of the word embeddings, the number of attention heads, the dimension of the hidden layer in the feed-forward network, the number of encoder blocks, and the dropout rate) affect the overall size of the model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "# Use different parameters to see the effects on the overall model size. \n",
        "# Create a Transformer Encoder with different hyperparameters\n",
        "new_model = TransformerEncoder(\"<FILL_IN>\")\n",
        "print(f\"The model has {count_parameters(model):,} trainable parameters, compared to the 21,986,304 of the original model\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion1_1(new_model)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 2:** \n",
        "Visualize the embeddings of a different set of words. Try choosing a set of words that are related in some way, such as words related to a particular topic or words that are all of a certain type (e.g., all verbs or all nouns)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "words = [\"<FILL_IN>\"]\n",
        "\n",
        "# Get the embeddings of the words\n",
        "embeddings = []\n",
        "for word in words:\n",
        "    inputs = tokenizer(word, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = <FILL_IN>\n",
        "    embeddings.append(outputs.last_hidden_state[0, 0, :].numpy())\n",
        "\n",
        "# Use PCA to reduce the dimensionality of the embeddings to 2\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(embeddings)\n",
        "\n",
        "# Plot the embeddings\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],s=500)\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion1_2(words)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 3:** \n",
        "Compute the cosine similarity between the embeddings of a sentence and its scrambled version. For example, compare the sentence \"the cat chased the dog\" with \"dog the chased cat the\". What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "# Define a sentence and its scrambled version\n",
        "sentence_q3 = \"the cat chased the dog\"\n",
        "scrambled_sentence_q3 = \"dog the chased cat the\"\n",
        "\n",
        "# Compute embeddings and cosine similarity\n",
        "original_embedding = \"<FILL_IN>\"\n",
        "scrambled_embedding = \"<FILL_IN>\"\n",
        "\n",
        "# Compute the average embeddings of each sentence\n",
        "avg_embedding_original = \"<FILL_IN>\"\n",
        "avg_embedding_scrambled = \"<FILL_IN>\"\n",
        "\n",
        "similarity = \"<FILL_IN>\"\n",
        "print(\"Cosine similarity between original and scrambled sentence embeddings:\", similarity.item())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion1_3(sentence_q3, scrambled_sentence_q3)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 4:** \n",
        "Compute the cosine similarity between the embeddings of a word used in two different contexts. For example, try the word `bank`, \"he needs to bank the money\" vs. \"he is going to the bank of the river\". What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "# Define two sentences where a word has different meanings\n",
        "sentence_q4a = \"he needs to bank the money\"\n",
        "sentence_q4b = \"he is going to the bank of the river\"\n",
        "# Compute embeddings and cosine similarity\n",
        "embedding1 = \"<FILL_IN>\"\n",
        "embedding2 = \"<FILL_IN>\"\n",
        "\n",
        "# Compute the average embeddings of each sentence\n",
        "avg_embedding1 = \"<FILL_IN>\"\n",
        "avg_embedding2 = \"<FILL_IN>\"\n",
        "\n",
        "similarity = \"<FILL_IN>\"\n",
        "print(f\"Cosine similarity between embeddings of the word 'bank' in two different contexts: {similarity.item()}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion1_4(sentence_q4a, sentence_q4b)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 2 - Masked Language Modeling (MLM) with BERT\n",
        "# MAGIC\n",
        "One of the training tasks for BERT is Masked Language Modeling (MLM). In MLM, some percentage of the input tokens are masked at random, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike traditional language modeling, MLM is not a sequential task because BERT uses the entire context (left and right of the mask) to predict the masked word. This allows BERT to pre-train a deep bidirectional representation in a straightforward way, which is difficult in standard language modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Masked Language Modeling with BERT\n",
        "# MAGIC\n",
        "We load a pre-trained BERT model that has been fine-tuned for the masked language modeling (MLM) task. We then define a function to predict the word that should fill a `[MASK]` token in a sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import the necessary libraries\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir=DA.paths.datasets+\"/models\")\n",
        "mlm_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\", cache_dir=DA.paths.datasets+\"/models\")\n",
        "\n",
        "# Define a function to predict masked words\n",
        "def predict_masked_words(sentence, model, tokenizer):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    predicted_token_ids = outputs.logits.argmax(dim=-1)\n",
        "    predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids[0])\n",
        "    return \" \".join(predicted_tokens)\n",
        "\n",
        "# Define a sentence with a masked word\n",
        "sentence = \"I love to play [MASK] ball.\"\n",
        "print(predict_masked_words(sentence, mlm_model, tokenizer))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predict Different Masked Words\n",
        "# MAGIC\n",
        "We define different sentences with masked words and use our model to predict what these masked words should be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define different sentences with masked words\n",
        "sentences = [\n",
        "    \"The weather today is [MASK].\",\n",
        "    \"I like to eat [MASK] for breakfast.\",\n",
        "    \"She is a [MASK] woman.\",\n",
        "    \"He drives a [MASK] car.\",\n",
        "    \"They are going to the [MASK].\"\n",
        "]\n",
        "\n",
        "# Use the model to predict the masked words\n",
        "for sentence in sentences:\n",
        "    print(predict_masked_words(sentence, mlm_model, tokenizer))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment with Different Sentences\n",
        "# MAGIC\n",
        "We define even more sentences with masked words and use our model to predict what these masked words should be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define more sentences with masked words\n",
        "sentences = [\n",
        "    \"My favorite color is [MASK].\",\n",
        "    \"The cat sat on the [MASK].\",\n",
        "    \"I am reading a [MASK] book.\",\n",
        "    \"She has a [MASK] of apples.\",\n",
        "    \"He plays the [MASK] in a band.\"\n",
        "]\n",
        "\n",
        "# Use the model to predict the masked words\n",
        "for sentence in sentences:\n",
        "    print(predict_masked_words(sentence, mlm_model, tokenizer))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 2 - Questions\n",
        "# MAGIC\n",
        "Having explored the use of BERT for masked language modeling, we can now explore some questions related to this task.\n",
        "# MAGIC\n",
        "**Question 5:** \n",
        "What happens when you mask more than one word in a sentence? Can the model accurately predict both masked words?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "# Define a sentence with two masked words\n",
        "sentence_q5 = \"<FILL_IN>\"\n",
        "print(predict_masked_words(sentence_q5, mlm_model, tokenizer))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion1_5(sentence_q5)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 6:** \n",
        "Use the model to predict the masked word in a sentence in a language other than English. Does the model accurately predict the masked word? Think about why/why not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "# Define a sentence in French with a masked word\n",
        "sentence_q6 = \"<FILL_IN>\"\n",
        "print(predict_masked_words(sentence, mlm_model, tokenizer))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion1_6(sentence_q6)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 7:** \n",
        "Mask a word that has different meanings in different contexts. Does the model accurately predict the correct word based on the context?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "# Define two sentences where a word has different meanings\n",
        "sentence_q7a = \"<FILL_IN>\"\n",
        "sentence_q7b = \"<FILL_IN>\"\n",
        "print(predict_masked_words(sentence_q7a, mlm_model, tokenizer))\n",
        "print(predict_masked_words(sentence_q7b, mlm_model, tokenizer))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion1_7(sentence_q7a, sentence_q7b)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 8:** \n",
        "Mask a word in a sentence that makes sense only in a specific cultural context. Does the model accurately predict the masked word?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "# Define a sentence that makes sense only in a specific cultural context\n",
        "sentence_q8 = \"<FILL_IN>\"\n",
        "print(predict_masked_words(sentence, mlm_model, tokenizer))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion1_8(sentence_q8)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question 9:** \n",
        "Mask a word in a sentence that contains an idiomatic expression. Does the model accurately predict the masked word?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "# Define a sentence that contains an idiomatic expression\n",
        "sentence_q9 = \"<FILL_IN>\"\n",
        "print(predict_masked_words(sentence_q9, mlm_model, tokenizer))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion1_9(sentence_q9)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-sandbox\n",
        "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
        "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
        "<br/>\n",
        "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}