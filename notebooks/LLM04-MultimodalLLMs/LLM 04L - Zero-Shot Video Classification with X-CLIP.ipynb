{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-sandbox\n",
        "# MAGIC\n",
        "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
        "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MAGIC\n",
        "# Zero-Shot Video Classification\n",
        "In this lab, we are going to pass a video of choice to [X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip) and ask X-CLIP to assign probabilities to the provided text description. This model developed by [Ni et al 2022](https://arxiv.org/abs/2208.02816) aims to extend OpenAI's CLIP model that's focused on image-related tasks. From Hugging Face's documentation:\n",
        "# MAGIC\n",
        ">The model consists of a text encoder, a cross-frame vision encoder, a multi-frame integration Transformer, and a video-specific prompt generator. \n",
        "# MAGIC\n",
        "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
        "1. You will learn how to load a video from YouTube and do minor processing on the video for X-CLIP \n",
        "1. Use X-CLIP to assign probabilities to text descriptions\n",
        "# MAGIC\n",
        "DISCLAIMER: The majority of this notebook's code is borrowed from Hugging Face's Tutorial GitHub Repo, specifically the[\"Transformers-Tutorials/X-CLIP\"](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/X-CLIP/Zero_shot_classify_a_YouTube_video_with_X_CLIP.ipynb) notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use [pytube](https://pytube.io/en/latest/index.html) to get videos from YouTube and load videos using [decord](https://github.com/dmlc/decord)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MAGIC %pip install decord==0.6.0 openai==0.27.8 pytube==15.0.0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MAGIC %run ../Includes/pytube_patch"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classroom Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MAGIC %run ../Includes/Classroom-Setup"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, we will load a YouTube video of a piano performance.\n",
        "# MAGIC\n",
        "`streams.filter` method provides flexible ways for us to filter based on the type of stream that we're interested in. Refer to [documentation here](https://pytube.io/en/latest/user/streams.html#filtering-by-streaming-method)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "# a video of piano performance \n",
        "youtube_url = \"https://www.youtube.com/watch?v=-xKM3mGt2pE\"\n",
        "yt = YouTube(youtube_url)\n",
        "\n",
        "streams = yt.streams.filter(file_extension=\"mp4\")\n",
        "print(streams)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's say that we only are interested in the first part of the video stream. We will download only the third portion and save it to our directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "output_dir = os.path.join(DA.paths.working_dir, \"video\")\n",
        "file_path = streams[0].download(output_path=output_dir)\n",
        "file_path"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recall from the presentation that audio data is often split into chunks. The same applies to videos as well. Below we will split the video into different frames. \n",
        "# MAGIC\n",
        "`frame_rate` is a common term in video processing to refer to # of pictures taken per second. For audio-only data, it's called `sampling_rate`.\n",
        "# MAGIC\n",
        "`VideoReader` helps us to access frames directly from the video files. Refer to [documentation here](https://github.com/dmlc/decord#videoreader)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from decord import VideoReader, cpu\n",
        "import torch\n",
        "import numpy as np\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# this does in-memory decoding of the video \n",
        "videoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n",
        "print(\"Length of video frames: \", len(videoreader))\n",
        "\n",
        "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
        "    \n",
        "    # Since each frame length is 4 seconds, we need to find the total frame length if we want `clip_len` frames \n",
        "    converted_len = int(clip_len * frame_sample_rate)\n",
        "\n",
        "    # Get a random frame to end on \n",
        "    end_idx = np.random.randint(converted_len, seg_len)\n",
        "    # Find the starting frame, if the frame has length of clip_len\n",
        "    start_idx = end_idx - converted_len\n",
        "\n",
        "    # np.linspace returns evenly spaced numbers over a specified interval \n",
        "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
        "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
        "    return indices"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1\n",
        "# MAGIC\n",
        "We want to retrieve 32 frames in total, with 4 seconds each. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO \n",
        "indices = sample_frame_indices(clip_len= <FILL_IN>, \n",
        "                               frame_sample_rate=<FILL_IN>, \n",
        "                               seg_len=len(videoreader))\n",
        "print(\"Number of frames we will retrieve: \", len(indices))\n",
        "\n",
        "# `get_batch` allows us to get multiple frames at once \n",
        "video = videoreader.get_batch(indices).asnumpy()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion4_1(indices)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now randomly pick a video frame to inspect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from PIL import Image\n",
        "\n",
        "Image.fromarray(video[8])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2\n",
        "# MAGIC\n",
        "We will now pass in XCLIP model to process our video frames and as our model to assign probabilities to text descriptions to the model. \n",
        "# MAGIC\n",
        "The model we will use is `microsoft/xclip-base-patch16-zero-shot`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO \n",
        "\n",
        "from transformers import XCLIPProcessor, XCLIPModel\n",
        "\n",
        "model_name = <FILL_IN>\n",
        "processor = XCLIPProcessor.from_pretrained(model_name)\n",
        "model = XCLIPModel.from_pretrained(model_name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion4_2(model_name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3 \n",
        "# MAGIC\n",
        "We will provide a list of three text descriptions and ask the model to assign probabitilies to each of them. \n",
        "# MAGIC\n",
        "Let's use `text_description_list = [\"play piano\", \"eat sandwich\", \"play football\"]` \n",
        "# MAGIC\n",
        "Hint:  for the `videos` argument: recall that we have a list of video frames we have processed in the cells above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO \n",
        "import torch\n",
        "\n",
        "text_description_list = <FILL_IN>\n",
        "\n",
        "inputs = processor(text=<FILL_IN>, \n",
        "                   videos=<FILL_IN>, \n",
        "                   return_tensors=\"pt\", \n",
        "                   padding=True)\n",
        "\n",
        "# forward pass\n",
        "# we are not going to train the model, hence we specify .no_grad()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# we will get probabilities per video frame and calculate the softmax \n",
        "video_probs = outputs.logits_per_video.softmax(dim=1)\n",
        "print(dict(zip(text_description_list, video_probs[0])))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test your answer. DO NOT MODIFY THIS CELL.\n",
        "\n",
        "dbTestQuestion4_3(text_description_list, video_probs)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Which text description has the highest probability? In the following optional section, you can play around with OpenAI's CLIP and Whisper API to generate image from text and get audio transcription."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OPTIONAL (Non-graded): Using OpenAI's CLIP and Whisper\n",
        "# MAGIC\n",
        "# MAGIC\n",
        "For this section to work, you need to generate an Open AI key. \n",
        "# MAGIC\n",
        "Steps:\n",
        "1. You need to [create an account](https://platform.openai.com/signup) on OpenAI. \n",
        "2. Generate an OpenAI [API key here](https://platform.openai.com/account/api-keys). \n",
        "# MAGIC\n",
        "Note: OpenAI does not have a free option, but it gives you $5 as credit. Once you have exhausted your $5 credit, you will need to add your payment method. You will be [charged per token usage](https://openai.com/pricing). **IMPORTANT**: It's crucial that you keep your OpenAI API key to yourself. If others have access to your OpenAI key, they will be able to charge their usage to your account! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<FILL IN>\""
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Using CLIP \n",
        "# MAGIC\n",
        "OpenAI's CLIP can help you generate images from provided text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "image_resp = openai.Image.create(prompt=\"robots play water balloons, modern painting\", \n",
        "                                 n=1, \n",
        "                                 size=\"512x512\")\n",
        "image_resp\n",
        "displayHTML(image_resp[\"data\"][0][\"url\"])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also use it to assign text caption probabilities based on provided image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "cat_image = Image.open(requests.get(url, stream=True).raw)\n",
        "display(cat_image)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "caption_list = [\"eating pasta\", \"cats sleeping\"]\n",
        "\n",
        "inputs = clip_processor(text=caption_list, \n",
        "                        images=cat_image, \n",
        "                        return_tensors=\"pt\", \n",
        "                        padding=True)\n",
        "\n",
        "clip_outputs = clip_model(**inputs)\n",
        "# This calculates image-text similarity score \n",
        "clip_logits_per_image = clip_outputs.logits_per_image \n",
        "\n",
        "# Use softmax to get caption probabilities \n",
        "image_probs = clip_logits_per_image.softmax(dim=1)\n",
        "print(dict(zip(caption_list, image_probs[0])))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see that the probability of the caption with \"cats\" is much higher than that of \"pasta\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Whisper\n",
        "# MAGIC\n",
        "OpenAI's Whisper Automatic Speech Recognition system is a simple and powerful tool for transcribing audio files. \n",
        "# MAGIC\n",
        "If you'd like to browse interesting Whisper applications that people have been exploring, visit [this link](https://github.com/openai/whisper/discussions/categories/show-and-tell), notably [this web UI application](https://huggingface.co/spaces/aadnk/whisper-webui) and [this transcription + speaker identification discussion](https://github.com/openai/whisper/discussions/264)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "\n",
        "# URL of the sample audio file (in this case, a simple English sentence)\n",
        "audio_url = \"https://audio-samples.github.io/samples/mp3/blizzard_primed/sample-1.mp3\"\n",
        "\n",
        "# Download the audio file\n",
        "response = requests.get(audio_url)\n",
        "\n",
        "audio_directory = os.path.join(DA.paths.working_dir, \"sample_audio.mp3\")\n",
        "# Save the audio file to disk\n",
        "with open(audio_directory, \"wb\") as audio_file:\n",
        "    audio_file.write(response.content) \n",
        "\n",
        "print(\"Sample audio file 'sample_audio.wav' downloaded.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "audio_file = open(audio_directory, \"rb\")\n",
        "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
        "print(transcript)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-sandbox\n",
        "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
        "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
        "<br/>\n",
        "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}