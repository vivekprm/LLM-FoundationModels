{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Tuning\n",
    "This lesson introduces how to apply prompt tuning to your model of choice using [Parameter-Efficient Fine-Tuning (PEFT) library developed by HuggingFace](https://huggingface.co/docs/peft/index). This PEFT library supports multiple methods to reduce the number of parameters for fine-tuning, including prompt tuning and LoRA. For a complete list of methods, refer to their [documentation](https://huggingface.co/docs/peft/main/en/index#supported-methods). Only a subset of models and tasks are supported by this PEFT library for the time being, including GPT-2, LLaMA; for pairs of models and tasks supported, refer to this [page](https://huggingface.co/docs/peft/main/en/index#supported-models).\n",
    "\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "1. Apply prompt tuning to your model of choice\n",
    "1. Fine-tune on your provided dataset\n",
    "1. Save and share your model to HuggingFace hub\n",
    "1. Conduct inference using the fine-tuned model\n",
    "1. Compare outputs from randomly- and text-initialized fine-tuned model vs. foundation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAGIC %pip install peft==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAGIC %run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Auto Classes](https://huggingface.co/docs/transformers/main/en/model_doc/auto#auto-classes) helps you automatically retrieve the relevant model and tokenizers, given the pre-trained models you are interested in using. \n",
    "\n",
    "Causal language modeling refers to the decoding process, where the model predicts the next token based on only the tokens on the left. The model cannot see the future tokens, unlike masked language models that have full access to tokens bidirectionally. A canonical example of a causal language model is GPT-2. You also hear causal language models being described as autoregresssive as well. \n",
    "\n",
    "API docs:\n",
    "* [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer)\n",
    "* [AutoModelForCausalLM](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForCausalLM)\n",
    "\n",
    "In this demo, we will be using `bigscience/bloomz-560m` as our **foundation** causal LM to generate text. You can read more about [`bloomz` model here](https://huggingface.co/bigscience/bloomz). It was trained on [multi-lingual dataset](https://huggingface.co/datasets/bigscience/xP3), spanning 46 languages and 13 programming langauges. The dataset covers a wide range of NLP tasks, including Q/A, title generation, text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5fdfae6d434e10a85c59b73f4ec7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)okenizer_config.json'), FloatProgress(value=0.0, max=222.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4228a3d1c04d3fb320e9af10e96ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading tokenizer.json'), FloatProgress(value=0.0, max=14500438.0), HTML(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027f6f8ef28c4468bd09eecab16c2ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)cial_tokens_map.json'), FloatProgress(value=0.0, max=85.0), HTML(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c05cf7932f415da7ea22747747f9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)lve/main/config.json'), FloatProgress(value=0.0, max=715.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b288a9888ede4bdf8c42e71a35faad02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading model.safetensors'), FloatProgress(value=0.0, max=1118459450.0), HTML(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"bigscience/bloomz-560m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing any fine-tuning, we will ask the model to generate a new phrase to the following input sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two things are infinite:  the number of people and the number']\n"
     ]
    }
   ],
   "source": [
    "input1 = tokenizer(\"Two things are infinite: \", return_tensors=\"pt\")\n",
    "\n",
    "foundation_outputs = foundation_model.generate(\n",
    "    input_ids=input1[\"input_ids\"], \n",
    "    attention_mask=input1[\"attention_mask\"], \n",
    "    max_new_tokens=7, \n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "print(tokenizer.batch_decode(foundation_outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is not too bad. However, the dataset BLOOMZ is pre-trained on doesn't cover anything about inspirational English quotes. Therefore, we are going to fine-tune `bloomz-560m` on [a dataset called `Abirate/english_quotes`](https://huggingface.co/datasets/Abirate/english_quotes)  containing exclusively inspirational English quotes, with the hopes of using the fine-tuned version to generate more quotes later! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-132931e50afa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Abirate/english_quotes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/datasets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"quote\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"Abirate/english_quotes\", cache_dir=DA.paths.datasets+\"/datasets\")\n",
    "\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "train_sample = data[\"train\"].select(range(50))\n",
    "display(train_sample) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onto fine-tuning: define PEFT configurations for random initialization\n",
    "Recall that prompt tuning allows both random and initialization of soft prompts or also known as virtual tokens. We will compare the model outputs from both initialization methods later. For now, we will start with random initialization, where all we provide is the length of the virtual prompt. \n",
    "\n",
    "API docs:\n",
    "* [PromptTuningConfig](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig)\n",
    "* [PEFT model](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import  get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.RANDOM,\n",
    "    num_virtual_tokens=4,\n",
    "    tokenizer_name_or_path=model_name\n",
    ")\n",
    "peft_model = get_peft_model(foundation_model, peft_config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the beauty of PEFT! It allows us to drastically reduce the number of trainable parameters. Now, we can proceed with using [HuggingFace's `Trainer` class](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#trainer) and its [`TrainingArugments` to define our fine-tuning configurations](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). \n",
    "\n",
    "The `Trainer` class provides user-friendly abstraction to leverage PyTorch under the hood to conduct training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import os\n",
    "\n",
    "output_directory = os.path.join(DA.paths.working_dir, \"peft_outputs\")\n",
    "\n",
    "if not os.path.exists(DA.paths.working_dir):\n",
    "    os.mkdir(DA.paths.working_dir)\n",
    "if not os.path.exists(output_directory):\n",
    "    os.mkdir(output_directory)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_directory, # Where the model predictions and checkpoints will be written\n",
    "    no_cuda=True, # This is necessary for CPU clusters. \n",
    "    auto_find_batch_size=True, # Find a suitable batch size that will fit into memory automatically \n",
    "    learning_rate= 3e-2, # Higher learning rate than full fine-tuning\n",
    "    num_train_epochs=5 # Number of passes to go through the entire fine-tuning dataset \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "We will also use `Data Collator` to help us form batches of inputs to pass in to the model for training. Go [here](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#data-collator) for documentation.\n",
    "\n",
    "Specifically, we will be using `DataCollatorforLanguageModeling` which will additionally pad the inputs to the maximum length of a batch since the inputs can have variable lengths. Refer to [API docs here](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling).\n",
    "\n",
    "Note: This cell might take ~10 mins to train. **Decrease `num_train_epochs` above to speed up the training process.** On another hand, you might notice that this cells triggers a whole new MLflow run. [MLflow](https://mlflow.org/docs/latest/index.html) is an open source tool that helps to manage end-to-end machine learning lifecycle, including experiment tracking, ML code packaging, and model deployment. You can read more about [LLM tracking here](https://mlflow.org/docs/latest/llm-tracking.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model, # We pass in the PEFT version of the foundation model, bloomz-560M\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False) # mlm=False indicates not to use masked language modeling\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_now = time.time()\n",
    "peft_model_path = os.path.join(output_directory, f\"peft_model_{time_now}\")\n",
    "trainer.model.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "You can load the model from the path that you have saved to before, and ask the model to generate text based on our input before! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "loaded_model = PeftModel.from_pretrained(foundation_model, \n",
    "                                         peft_model_path, \n",
    "                                         is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_outputs = loaded_model.generate(\n",
    "    input_ids=input1[\"input_ids\"], \n",
    "    attention_mask=input1[\"attention_mask\"], \n",
    "    max_new_tokens=7, \n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "print(tokenizer.batch_decode(loaded_model_outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it seems like our fine-tuned model is indeed getting closer to generating inspirational quotes. \n",
    "\n",
    "In fact, the input above is taken from the training dataset. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/llm/english_quote_example.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text initialization\n",
    "Our fine-tuned, randomly initialized model did pretty well on the quote above. Let's now compare it with the text initialization method. \n",
    "\n",
    "Notice that all we are changing is the `prompt_tuning_init` setting and we are also providing a concise text prompt. \n",
    "\n",
    "API docs\n",
    "* [prompt_tuning_init_text](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig.prompt_tuning_init_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    prompt_tuning_init_text=\"Generate inspirational quotes\", # this provides a starter for the model to start searching for the best embeddings\n",
    "    num_virtual_tokens=3, # this doesn't have to match the length of the text above\n",
    "    tokenizer_name_or_path=model_name\n",
    ")\n",
    "text_peft_model = get_peft_model(foundation_model, text_peft_config)\n",
    "print(text_peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_trainer = Trainer(\n",
    "    model=text_peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "text_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "time_now = time.time()\n",
    "text_peft_model_path = os.path.join(output_directory, f\"text_peft_model_{time_now}\")\n",
    "text_trainer.model.save_pretrained(text_peft_model_path)\n",
    "\n",
    "# Load model \n",
    "loaded_text_model = PeftModel.from_pretrained(\n",
    "    foundation_model, \n",
    "    text_peft_model_path, \n",
    "    is_trainable=False\n",
    ")\n",
    "\n",
    "# Generate output\n",
    "text_outputs = text_peft_model.generate(\n",
    "    input_ids=input1[\"input_ids\"], \n",
    "    attention_mask=input1[\"attention_mask\"], \n",
    "    max_new_tokens=7, \n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "    \n",
    "print(tokenizer.batch_decode(text_outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that text initialization doesn't necessarily perform better than random initialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Share model to HuggingFace hub (optional)\n",
    "If you have a model that you would like to share with the rest of the HuggingFace community, you can choose to push your model to the HuggingFace hub! \n",
    "\n",
    "1. You need to first create a free HuggingFace account! The signup process is simple. Go to the [home page](https://huggingface.co/) and click \"Sign Up\" on the top right corner.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/llm/hf_homepage_signup.png\" width=700>\n",
    "\n",
    "2. Once you have signed up and confirmed your email address, click on your user icon on the top right and click the `Settings` button. \n",
    "\n",
    "3. Navigate to the `Access Token` tab and copy your token. \n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/llm/hf_token_page.png\" width=500>\n",
    "# MAGIC\n",
    "# MAGIC\n",
    "# MAGIC\n",
    "API docs:\n",
    "* [push_to_hub](https://huggingface.co/docs/transformers/main/en/model_sharing#share-a-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use Databricks Secrets management tool to save your secrets to a secret scope on Databricks. For more documentation on how to manage secrets on Databricks, visit this page on [secret management](https://docs.databricks.com/en/security/secrets/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "os.environ[\"huggingface_key\"] = dbutils.secrets.get(\"llm_scope\", \"huggingface_key\")\n",
    "hf_token = os.environ[\"huggingface_key\"]\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use HuggingFace's helper login method. This login cell below will prompt you to enter your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "hf_username = <FILL_IN_WITH_YOUR_HUGGINGFACE_USERNAME>\n",
    "peft_model_id = f\"{hf_username}/bloom_prompt_tuning_{time_now}\"\n",
    "trainer.model.push_to_hub(peft_model_id, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference from model in HuggingFace hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path)\n",
    "peft_random_model = PeftModel.from_pretrained(foundation_model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_model_outputs = peft_random_model.generate(\n",
    "    input_ids=input1[\"input_ids\"], \n",
    "    attention_mask=input1[\"attention_mask\"], \n",
    "    max_new_tokens=7, \n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "print(tokenizer.batch_decode(online_model_outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats on applying PEFT - prompt tuning for the first time! In the lab notebook, you will be applying LoRA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-sandbox\n",
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
